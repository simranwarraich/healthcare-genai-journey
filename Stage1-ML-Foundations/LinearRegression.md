QC Notes: Supervised Learning â€“ Linear Regression

1. Supervised Learning

Definition: Machine learning where the model learns from labeled data (input X, output Y).

Goal: Map inputs â†’ outputs by minimizing error.

Examples: Predicting house prices, salary prediction, medical cost prediction.

2. Linear Regression

Purpose: Predict a continuous (numeric) output.

Equation:   Y=Î²0â€‹+Î²1â€‹X+Ïµ

Y: dependent variable (target)
X: independent variable (feature)
ğ›½0: intercept
ğ›½1: slope (effect of X on Y)
Ïµ: error term

3. Types

Simple Linear Regression â€“ one independent variable.

Multiple Linear Regression â€“ multiple independent variables.

4. Assumptions

Linearity (relationship between X and Y is linear).

Independence of errors.

Homoscedasticity (constant variance of errors).

Normal distribution of errors.

No multicollinearity (for multiple regression).

5. Model Fitting
Method: Ordinary Least Squares (OLS) â†’ minimizes the sum of squared errors (SSE).
SSE=âˆ‘(Yiâ€‹âˆ’Y^iâ€‹)2
6. Performance Metrics

RÂ² (Coefficient of Determination): Proportion of variance explained.

Adjusted RÂ²: Adjusts for number of predictors.

MSE / RMSE: Mean squared error / Root MSE.

MAE: Mean absolute error.

7. Pros

Simple to implement, fast.

Easy to interpret (clear relationship between variables).

Works well with linear relationships.

8. Cons

Sensitive to outliers.

Poor with non-linear data.

Assumptions often violated in real-world data.

9. Applications

Predicting housing prices.

Sales forecasting.

Medical cost estimation.

Risk prediction in insurance.
